{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W207_Final_Project_Yu_Zhu_Lam.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ev7E5YHRcss-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tOphhU8lrej"
      },
      "source": [
        "# OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnqSDG-RtOWE"
      },
      "source": [
        "Our team is interested in tackling the OpenVaccine Kaggle competition (https://www.kaggle.com/c/stanford-covid-vaccine) by developing machine learning models to explore the dataset and to predict COVID-19 mRNA vaccine degradation. Train set provides 2400 RNA sequences, and each sequence has three RNA structure features (structure, predicted loop type, base pairing probability) and five ground truths that evaluated through experiments (deg_pH10, deg_Mg_pH10, deg_50C, deg_Mg_50C, reactivity) and their correponding errors. The goal of this competiton is to use RNA structural features to predict the degradation rates at each position of RNA sequences at multiple experimental conditions.\n",
        "\n",
        "RNA molecules is encoded by four nucleic acids Adenine (A), Guanine (G), Cytosine(C), and Uracil(U). The permutation and combination of the nucleic acids forms three dimentional secondary structures, such as hairpin and loop by pairing themselves through A-U and G-C pairs. The difference in loop types directly affects the degradation rate at each base position. For example, a hairpin will form when multiple bases pair with others forming hydrogen bonds, and become less likely to degrade under challenge conditions such as treatment at pH 10. In the RNA research field, RNA 3D structure is very difficult to be determined experimentally or predict computationally. The features predicted_loop_type and structure in this data set are both predicted through bioinformatics tools; therefore, given the potential errors embedded with the featrues, this may inevitably affect our models and its result. \n",
        "\n",
        "The additional challenge in this exercise is the different size of training and testing target. In the 'private' testing dataset, both the feature array length and the output array length is different from the the training data set. Although some of the algorithm we trained permit variable data length, we have decided to focus on the 'public' testing data, which has the same dimension as the training data so we can explore variations of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgSSOSMqYhh"
      },
      "source": [
        "## Initial loading "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsCOhmCltlqv"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4cYnkmN2EHA"
      },
      "source": [
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "\n",
        "# General libraries.\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "# SK-learn libraries for learning.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Tools for counting letters in the sequences\n",
        "from collections import Counter as count\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# import tenserflow for neural network models\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, GRU\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.metrics import categorical_crossentropy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIiq1g_GZfh8"
      },
      "source": [
        "### Mounting Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2kpYO_Z4kqr",
        "outputId": "ace86ffb-4fe5-4dae-e458-2898e89f0e88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paU8tT5NZR2F"
      },
      "source": [
        "### Load the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQubGQsNltFG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "174a27bc-2efb-4b98-cc19-1d5352858abd"
      },
      "source": [
        "full_train = pd.read_json('/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/train.json', lines=True)\n",
        "full_train = full_train.set_index(keys='index')\n",
        "full_train.info()\n",
        "full_train.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2400 entries, 0 to 2399\n",
            "Data columns (total 18 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   id                   2400 non-null   object \n",
            " 1   sequence             2400 non-null   object \n",
            " 2   structure            2400 non-null   object \n",
            " 3   predicted_loop_type  2400 non-null   object \n",
            " 4   signal_to_noise      2400 non-null   float64\n",
            " 5   SN_filter            2400 non-null   int64  \n",
            " 6   seq_length           2400 non-null   int64  \n",
            " 7   seq_scored           2400 non-null   int64  \n",
            " 8   reactivity_error     2400 non-null   object \n",
            " 9   deg_error_Mg_pH10    2400 non-null   object \n",
            " 10  deg_error_pH10       2400 non-null   object \n",
            " 11  deg_error_Mg_50C     2400 non-null   object \n",
            " 12  deg_error_50C        2400 non-null   object \n",
            " 13  reactivity           2400 non-null   object \n",
            " 14  deg_Mg_pH10          2400 non-null   object \n",
            " 15  deg_pH10             2400 non-null   object \n",
            " 16  deg_Mg_50C           2400 non-null   object \n",
            " 17  deg_50C              2400 non-null   object \n",
            "dtypes: float64(1), int64(3), object(14)\n",
            "memory usage: 356.2+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sequence</th>\n",
              "      <th>structure</th>\n",
              "      <th>predicted_loop_type</th>\n",
              "      <th>signal_to_noise</th>\n",
              "      <th>SN_filter</th>\n",
              "      <th>seq_length</th>\n",
              "      <th>seq_scored</th>\n",
              "      <th>reactivity_error</th>\n",
              "      <th>deg_error_Mg_pH10</th>\n",
              "      <th>deg_error_pH10</th>\n",
              "      <th>deg_error_Mg_50C</th>\n",
              "      <th>deg_error_50C</th>\n",
              "      <th>reactivity</th>\n",
              "      <th>deg_Mg_pH10</th>\n",
              "      <th>deg_pH10</th>\n",
              "      <th>deg_Mg_50C</th>\n",
              "      <th>deg_50C</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_001f94081</td>\n",
              "      <td>GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...</td>\n",
              "      <td>.....((((((.......)))).)).((.....((..((((((......</td>\n",
              "      <td>EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...</td>\n",
              "      <td>6.894</td>\n",
              "      <td>1</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "      <td>[0.1359, 0.20700000000000002, 0.1633, 0.1452, ...</td>\n",
              "      <td>[0.26130000000000003, 0.38420000000000004, 0.1...</td>\n",
              "      <td>[0.2631, 0.28600000000000003, 0.0964, 0.1574, ...</td>\n",
              "      <td>[0.1501, 0.275, 0.0947, 0.18660000000000002, 0...</td>\n",
              "      <td>[0.2167, 0.34750000000000003, 0.188, 0.2124, 0...</td>\n",
              "      <td>[0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...</td>\n",
              "      <td>[0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...</td>\n",
              "      <td>[2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...</td>\n",
              "      <td>[0.35810000000000003, 2.9683, 0.2589, 1.4552, ...</td>\n",
              "      <td>[0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_0049f53ba</td>\n",
              "      <td>GGAAAAAGCGCGCGCGGUUAGCGCGCGCUUUUGCGCGCGCUGUACC...</td>\n",
              "      <td>.....(((((((((((((((((((((((....)))))))))).)))...</td>\n",
              "      <td>EEEEESSSSSSSSSSSSSSSSSSSSSSSHHHHSSSSSSSSSSBSSS...</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "      <td>[2.8272, 2.8272, 2.8272, 4.7343, 2.5676, 2.567...</td>\n",
              "      <td>[73705.3985, 73705.3985, 73705.3985, 73705.398...</td>\n",
              "      <td>[10.1986, 9.2418, 5.0933, 5.0933, 5.0933, 5.09...</td>\n",
              "      <td>[16.6174, 13.868, 8.1968, 8.1968, 8.1968, 8.19...</td>\n",
              "      <td>[15.4857, 7.9596, 13.3957, 5.8777, 5.8777, 5.8...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 2.2965, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[4.947, 4.4523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[4.8511, 4.0426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[7.6692, 0.0, 10.9561, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_006f36f57</td>\n",
              "      <td>GGAAAGUGCUCAGAUAAGCUAAGCUCGAAUAGCAAUCGAAUAGAAU...</td>\n",
              "      <td>.....((((.((.....((((.(((.....)))..((((......)...</td>\n",
              "      <td>EEEEESSSSISSIIIIISSSSMSSSHHHHHSSSMMSSSSHHHHHHS...</td>\n",
              "      <td>8.800</td>\n",
              "      <td>1</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "      <td>[0.0931, 0.13290000000000002, 0.11280000000000...</td>\n",
              "      <td>[0.1365, 0.2237, 0.1812, 0.1333, 0.1148, 0.160...</td>\n",
              "      <td>[0.17020000000000002, 0.178, 0.111, 0.091, 0.0...</td>\n",
              "      <td>[0.1033, 0.1464, 0.1126, 0.09620000000000001, ...</td>\n",
              "      <td>[0.14980000000000002, 0.1761, 0.1517, 0.116700...</td>\n",
              "      <td>[0.44820000000000004, 1.4822, 1.1819, 0.743400...</td>\n",
              "      <td>[0.2504, 1.4021, 0.9804, 0.49670000000000003, ...</td>\n",
              "      <td>[2.243, 2.9361, 1.0553, 0.721, 0.6396000000000...</td>\n",
              "      <td>[0.5163, 1.6823000000000001, 1.0426, 0.7902, 0...</td>\n",
              "      <td>[0.9501000000000001, 1.7974999999999999, 1.499...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_0082d463b</td>\n",
              "      <td>GGAAAAGCGCGCGCGCGCGCGCGAAAAAGCGCGCGCGCGCGCGCGC...</td>\n",
              "      <td>......((((((((((((((((......))))))))))))))))((...</td>\n",
              "      <td>EEEEEESSSSSSSSSSSSSSSSHHHHHHSSSSSSSSSSSSSSSSSS...</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "      <td>[3.5229, 6.0748, 3.0374, 3.0374, 3.0374, 3.037...</td>\n",
              "      <td>[73705.3985, 73705.3985, 73705.3985, 73705.398...</td>\n",
              "      <td>[11.8007, 12.7566, 5.7733, 5.7733, 5.7733, 5.7...</td>\n",
              "      <td>[121286.7181, 121286.7182, 121286.7181, 121286...</td>\n",
              "      <td>[15.3995, 8.1124, 7.7824, 7.7824, 7.7824, 7.78...</td>\n",
              "      <td>[0.0, 2.2399, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "      <td>[0.0, -0.5083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
              "      <td>[3.4248, 6.8128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[0.0, -0.8365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
              "      <td>[7.6692, -1.3223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0087940f4</td>\n",
              "      <td>GGAAAAUAUAUAAUAUAUUAUAUAAAUAUAUUAUAGAAGUAUAAUA...</td>\n",
              "      <td>.....(((((((.((((((((((((.(((((((((....)))))))...</td>\n",
              "      <td>EEEEESSSSSSSBSSSSSSSSSSSSBSSSSSSSSSHHHHSSSSSSS...</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "      <td>[1.665, 2.1728, 2.0041, 1.2405, 0.620200000000...</td>\n",
              "      <td>[4.2139, 3.9637000000000002, 3.2467, 2.4716, 1...</td>\n",
              "      <td>[3.0942, 3.015, 2.1212, 2.0552, 0.881500000000...</td>\n",
              "      <td>[2.6717, 2.4818, 1.9919, 2.5484999999999998, 1...</td>\n",
              "      <td>[1.3285, 3.6173, 1.3057, 1.3021, 1.1507, 1.150...</td>\n",
              "      <td>[0.8267, 2.6577, 2.8481, 0.40090000000000003, ...</td>\n",
              "      <td>[2.1058, 3.138, 2.5437000000000003, 1.0932, 0....</td>\n",
              "      <td>[4.7366, 4.6243, 1.2068, 1.1538, 0.0, 0.0, 0.7...</td>\n",
              "      <td>[2.2052, 1.7947000000000002, 0.7457, 3.1233, 0...</td>\n",
              "      <td>[0.0, 5.1198, -0.3551, -0.3518, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ...                                            deg_50C\n",
              "index                ...                                                   \n",
              "0      id_001f94081  ...  [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...\n",
              "1      id_0049f53ba  ...  [7.6692, 0.0, 10.9561, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "2      id_006f36f57  ...  [0.9501000000000001, 1.7974999999999999, 1.499...\n",
              "3      id_0082d463b  ...  [7.6692, -1.3223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "4      id_0087940f4  ...  [0.0, 5.1198, -0.3551, -0.3518, 0.0, 0.0, 0.0,...\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsiUXDsHpxqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bd4298-ff25-4981-a093-32fa7f5a588d"
      },
      "source": [
        "print(full_train.columns)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'sequence', 'structure', 'predicted_loop_type', 'signal_to_noise',\n",
            "       'SN_filter', 'seq_length', 'seq_scored', 'reactivity_error',\n",
            "       'deg_error_Mg_pH10', 'deg_error_pH10', 'deg_error_Mg_50C',\n",
            "       'deg_error_50C', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C',\n",
            "       'deg_50C'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iU5YK_-ZjZX"
      },
      "source": [
        "### Load the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6A548tNCC-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "c3b191b8-0884-4b70-c4a0-40cdd3b2f554"
      },
      "source": [
        "full_test = pd.read_json('/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/test.json', lines=True)\n",
        "full_test = full_test.set_index(keys='index')\n",
        "full_test.info()\n",
        "full_test.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3634 entries, 0 to 3633\n",
            "Data columns (total 6 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   id                   3634 non-null   object\n",
            " 1   sequence             3634 non-null   object\n",
            " 2   structure            3634 non-null   object\n",
            " 3   predicted_loop_type  3634 non-null   object\n",
            " 4   seq_length           3634 non-null   int64 \n",
            " 5   seq_scored           3634 non-null   int64 \n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 198.7+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sequence</th>\n",
              "      <th>structure</th>\n",
              "      <th>predicted_loop_type</th>\n",
              "      <th>seq_length</th>\n",
              "      <th>seq_scored</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_00073f8be</td>\n",
              "      <td>GGAAAAGUACGACUUGAGUACGGAAAACGUACCAACUCGAUUAAAA...</td>\n",
              "      <td>......((((((((((.(((((.....))))))))((((((((......</td>\n",
              "      <td>EEEEEESSSSSSSSSSBSSSSSHHHHHSSSSSSSSSSSSSSSSHHH...</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_000ae4237</td>\n",
              "      <td>GGAAACGGGUUCCGCGGAUUGCUGCUAAUAAGAGUAAUCUCUAAAU...</td>\n",
              "      <td>.....((((..((((((...(((((.....((((....)))).......</td>\n",
              "      <td>EEEEESSSSIISSSSSSIIISSSSSIIIIISSSSHHHHSSSSIIII...</td>\n",
              "      <td>130</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_00131c573</td>\n",
              "      <td>GGAAAACAAAACGGCCUGGAAGACGAAGGAAUUCGGCGCGAAGGCC...</td>\n",
              "      <td>...........((.(((.(.(..((..((..((((...))))..))...</td>\n",
              "      <td>EEEEEEEEEEESSISSSISISIISSIISSIISSSSHHHSSSSIISS...</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_00181fd34</td>\n",
              "      <td>GGAAAGGAUCUCUAUCGAAGGAUAGAGAUCGCUCGCGACGGCACGA...</td>\n",
              "      <td>......((((((((((....))))))))))((((((..((.(((.....</td>\n",
              "      <td>EEEEEESSSSSSSSSSHHHHSSSSSSSSSSSSSSSSIISSISSSHH...</td>\n",
              "      <td>107</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_0020473f7</td>\n",
              "      <td>GGAAACCCGCCCGCGCCCGCCCGCGCUGCUGCCGUGCCUCCUCUCC...</td>\n",
              "      <td>.....(((((((((((((((((((((((((((((((((((((((((...</td>\n",
              "      <td>EEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...</td>\n",
              "      <td>130</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... seq_scored\n",
              "index                ...           \n",
              "0      id_00073f8be  ...         68\n",
              "1      id_000ae4237  ...         91\n",
              "2      id_00131c573  ...         68\n",
              "3      id_00181fd34  ...         68\n",
              "4      id_0020473f7  ...         91\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MQ-V55TFRt2"
      },
      "source": [
        "### Load BPPS files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jqR82sTFLXQ"
      },
      "source": [
        "def load_bpps(df):\n",
        "  bpps = []\n",
        "  for seq_id in df.id.to_list():\n",
        "    bpps.append(np.load(f'/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/bpps/{seq_id}.npy'))\n",
        "  return bpps"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU4MmV-95kOb"
      },
      "source": [
        "def bpps_avg(df):\n",
        "  bpps_arr = []\n",
        "  for seq_id in df.id.to_list():\n",
        "    bpps_arr.append(np.load(f'/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/bpps/{seq_id}.npy').mean(axis=1))\n",
        "  return bpps_arr"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPtdt5SuqgDA"
      },
      "source": [
        "def bpps_full(df):\n",
        "  bpps_arr = []\n",
        "  for seq_id in df.id.to_list():\n",
        "    bpps_arr.append(np.load(f'/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/bpps/{seq_id}.npy'))\n",
        "  return bpps_arr"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q_Nhff0rhy7"
      },
      "source": [
        "# add bpps data to the full train / test\n",
        "full_train[\"bpps_average\"] = bpps_avg(full_train)\n",
        "full_test[\"bpps_average\"] = bpps_avg(full_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpWIsZwSqpAc"
      },
      "source": [
        "# add full bpps data to the full train / test\n",
        "full_train[\"bpps_full\"] = bpps_full(full_train)\n",
        "full_test[\"bpps_full\"] = bpps_full(full_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSeVn7njvjVN"
      },
      "source": [
        "full_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E85R52RR7cgn"
      },
      "source": [
        "# export data set to csv to avoid reload bpps file, which takes 30 mins.\n",
        "# full_train.to_csv(\"/content/drive/MyDrive/Final Project/full_train_w_bpps.csv\")\n",
        "# full_test.to_csv(\"/content/drive/MyDrive/Final Project/full_test_w_bpps.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYfBDlVCd1T"
      },
      "source": [
        "# #load the file with bpps from google drive\n",
        "# full_train_w_bpps = pd.read_csv(\"/content/drive/MyDrive/Final Project/full_train_w_bpps.csv\")\n",
        "# full_test_w_bpps = pd.read_csv(\"/content/drive/MyDrive/Final Project/full_test_w_bpps.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfYne_DdFP8u"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhwZJBHZyCz"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXeiNNbVEqCg"
      },
      "source": [
        "First, we start with sanity check. According to competition description, the test set is a mixture of 'private test' and 'public test' data, differ by the length of RNA sequence (seq_length). Here we separate the test set into two sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CFi_48DCHgW"
      },
      "source": [
        "public_test = full_test[full_test.seq_length==107].reset_index()\n",
        "private_test = full_test[full_test.seq_length==130].reset_index()\n",
        "print(public_test.shape)\n",
        "print(private_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqrgz8Ia2f0T"
      },
      "source": [
        "Checking the training data as with quick descrption.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhMqSceC4OrT"
      },
      "source": [
        "include =['object', 'float', 'int']\n",
        "descriptive_summary = full_train.describe(include = include)\n",
        "\n",
        "descriptive_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Eqv3tl-J4Q"
      },
      "source": [
        "The data is structured quite complicated with different length for sequences can be different in different data set, so just double check that length is expected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6II4mYg8wz5"
      },
      "source": [
        "def check_seq_length (data,seq, expected_length):\n",
        "  return data.apply(lambda x:len(x[seq]) == x[expected_length], axis = 1)\n",
        "\n",
        "for seq in ['sequence', 'structure', 'predicted_loop_type']: \n",
        "  print(f'Is the length for {seq} as expected?', all(check_seq_length(full_train, seq, 'seq_length')))\n",
        "\n",
        "\n",
        "for seq in ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']: \n",
        "  print(f'Is the length for {seq} as expected?', all(check_seq_length(full_train, seq, 'seq_scored')))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sSeGOxvZ2DJ"
      },
      "source": [
        "# Function to plot correlation matricies\n",
        "def corrdot(*args, **kwargs):\n",
        "    corr_r = args[0].corr(args[1], 'pearson')\n",
        "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
        "    ax = plt.gca()\n",
        "    ax.set_axis_off()\n",
        "    marker_size = abs(corr_r) * 10000\n",
        "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
        "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
        "    font_size = abs(corr_r) * 40 + 5\n",
        "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
        "                ha='center', va='center', fontsize=font_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju9gZ8BB2EHO"
      },
      "source": [
        "RNA contains four bases and form G-C and A-U paris. COVID-19 mRNA vaccine is single-stranded RNA and it forms pairs as it floats and folds itself. Among the two pair types, G-C pair tends to be more statble than A-U pair due to the bonding strength. Therefore, the higher weight of G and C bases in a RNA will make RNA less degradable. Furthermore, structure and loop type also affects RNA degradation. Unpaired bases are more vulnerable to temperature and UV challenges. Stem stucture tend to be more stable tham loops.\n",
        "Below, we are exploring the weight of each base in each RNA sequence, weight of possible pairings, and weight of different structure types. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzrKeeBfadcn"
      },
      "source": [
        "What is the fraction of each base in each observation? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVmA-NV7aOUn"
      },
      "source": [
        "# Count the fraction of all the bases in the sequences\n",
        "bases = []\n",
        "\n",
        "for base in range(len(full_train)):\n",
        "    counts = dict(count(full_train.iloc[base]['sequence']))\n",
        "    bases.append((\n",
        "        counts['A'] / 107,\n",
        "        counts['G'] / 107,\n",
        "        counts['C'] / 107,\n",
        "        counts['U'] / 107\n",
        "    ))\n",
        "    \n",
        "bases = pd.DataFrame(bases, columns=['A_percent', 'G_percent', 'C_percent', 'U_percent'])\n",
        "bases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzUfV8NvAaUx"
      },
      "source": [
        "include =['object', 'float', 'int']\n",
        "base_summary = bases.describe(include = include)\n",
        "\n",
        "base_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80s78GrHAorR"
      },
      "source": [
        "Of the 4 bases, A has the highest average prevalence while U is the least prevalent on average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6xBZmICbBzd"
      },
      "source": [
        "What is fraction of each base pair in each observation? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z7YbQ1SbF3J"
      },
      "source": [
        "pairs = []\n",
        "all_partners = []\n",
        "for base in range(len(full_train)):\n",
        "  partners = [-1 for symbol in range(130)]\n",
        "  pairs_dict = {('U', 'G'): 0, ('C', 'G'): 0, ('U', 'A'): 0, ('G', 'C'): 0, ('A', 'U'): 0, ('G', 'U'): 0}\n",
        "  queue = []\n",
        "  for symbol in range(0, len(full_train.iloc[base]['structure'])):\n",
        "    if full_train.iloc[base]['structure'][symbol] == '(':\n",
        "      queue.append(symbol)\n",
        "    if full_train.iloc[base]['structure'][symbol] == ')':\n",
        "      first = queue.pop()\n",
        "      pairs_dict[(full_train.iloc[base]['sequence'][first], full_train.iloc[base]['sequence'][symbol])] += 1\n",
        "      partners[first] = symbol\n",
        "      partners[symbol] = first\n",
        "  \n",
        "  all_partners.append(partners)\n",
        "  \n",
        "  pairs_num = 0\n",
        "  pairs_unique = [('U', 'G'), ('C', 'G'), ('U', 'A'), ('G', 'C'), ('A', 'U'), ('G', 'U')]\n",
        "  for item in pairs_dict:\n",
        "    pairs_num += pairs_dict[item]\n",
        "  add_tuple = list()\n",
        "  for item in pairs_unique:\n",
        "    add_tuple.append(pairs_dict[item]/pairs_num)\n",
        "  pairs.append(add_tuple)\n",
        "    \n",
        "pairs = pd.DataFrame(pairs, columns=['U-G', 'C-G', 'U-A', 'G-C', 'A-U', 'G-U'])\n",
        "full_train['partners'] = all_partners\n",
        "pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBTQIMptA7I6"
      },
      "source": [
        "include =['object', 'float', 'int']\n",
        "pair_summary = pairs.describe(include = include)\n",
        "\n",
        "pair_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr_MrH85BeUt"
      },
      "source": [
        "Looking at the base pairs, on average, the G-C pair is the most prevalent while the U-G pair is the least"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQ8lhElceHy"
      },
      "source": [
        "What are the total counts of base pairs in the whole training set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY3uLy-RcccQ"
      },
      "source": [
        "pairs_dict = {('U', 'G'): 0, ('C', 'G'): 0, ('U', 'A'): 0, ('G', 'C'): 0, ('A', 'U'): 0, ('G', 'U'): 0}\n",
        "queue = []\n",
        "for base in range(len(full_train)):\n",
        "  observation = full_train.iloc[base]\n",
        "  for symbol in range(len(observation['structure'])):\n",
        "    if observation['structure'][symbol] == '(':\n",
        "      queue.append(symbol)\n",
        "    if observation['structure'][symbol] == ')':\n",
        "      first = queue.pop()\n",
        "      pairs_dict[(observation['sequence'][first], observation['sequence'][symbol])] += 1\n",
        "\n",
        "                \n",
        "pairs_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34sYeLLckFI"
      },
      "source": [
        "names = []\n",
        "values = []\n",
        "for item in pairs_dict:\n",
        "    names.append(item)\n",
        "    values.append(pairs_dict[item])\n",
        "    \n",
        "df = pd.DataFrame()\n",
        "df['pair'] = names\n",
        "df['count'] = values\n",
        "df['pair'] = df['pair'].astype(str)\n",
        "\n",
        "fig = px.bar(\n",
        "    df, \n",
        "    x='pair', \n",
        "    y=\"count\", \n",
        "    orientation='v', \n",
        "    title='Pair types', \n",
        "    height=400, \n",
        "    width=800\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DdTNwiacq1W"
      },
      "source": [
        "In the predicted loop type column, the following symbols were used to represent\n",
        "base pair categories.\n",
        "\n",
        "S: paired \"Stem\"\n",
        "\n",
        "M: Multiloop\n",
        "\n",
        "I: Internal loop \n",
        "\n",
        "B: Bulge\n",
        "\n",
        "H: Hairpin loop \n",
        "\n",
        "E: dangling End \n",
        "\n",
        "X: eXternal loop \n",
        "\n",
        "What is the fraction of each category in each observation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beRFtkzxcp8l"
      },
      "source": [
        "loops = []\n",
        "for prediction in range(len(full_train)):\n",
        "    counts = dict(count(full_train.iloc[prediction]['predicted_loop_type']))\n",
        "    types = ['E', 'S', 'H', 'B', 'X', 'I', 'M']\n",
        "    row = []\n",
        "    for item in types:\n",
        "      if item in counts:\n",
        "        row.append(counts[item] / 107)\n",
        "      else:\n",
        "        row.append(0)\n",
        "    loops.append(row)\n",
        "    \n",
        "loops = pd.DataFrame(loops, columns=types)\n",
        "loops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDgqUUgqFeKI"
      },
      "source": [
        "Looking at the loop type summaries, on average, S is the most prevalent type at 44.21% and B is the least present at 1.12%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZZ-h0exFdXG"
      },
      "source": [
        "include =['object', 'float', 'int']\n",
        "loops_summary = loops.describe(include = include)\n",
        "\n",
        "loops_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raaG8q0gdQcl"
      },
      "source": [
        "res_dict = {'E':0, 'S':0, 'H':0, 'B':0, 'X':0, 'I':0, 'M':0}\n",
        "for prediction in range(len(full_train)):\n",
        "    observation = full_train.iloc[prediction]\n",
        "    pred_symbol = dict(count(observation['predicted_loop_type']))\n",
        "    for item in pred_symbol:\n",
        "      if item in pred_symbol:\n",
        "        res_dict[item] += pred_symbol[item]\n",
        "\n",
        "res_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnOmrdpidT4N"
      },
      "source": [
        "names = []\n",
        "values = []\n",
        "for item in res_dict:\n",
        "    names.append(item)\n",
        "    values.append(res_dict[item])\n",
        "    \n",
        "df = pd.DataFrame()\n",
        "df['loop_type'] = names\n",
        "df['count'] = values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbkr8rpPdWH8"
      },
      "source": [
        "fig = px.bar(\n",
        "    df, \n",
        "    x='loop_type', \n",
        "    y=\"count\", \n",
        "    orientation='v', \n",
        "    title='Predicted loop types', \n",
        "    height=400, \n",
        "    width=600\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-SwQGyrdZII"
      },
      "source": [
        "#pairs and loops\n",
        "pairs.reset_index(drop=True, inplace=True)\n",
        "loops.reset_index(drop=True, inplace=True)\n",
        "\n",
        "correlation_frame = pd.concat([pairs, loops], axis=1)\n",
        "correlation_frame\n",
        "\n",
        "g = sns.PairGrid(correlation_frame, aspect=1.4, diag_sharey=False)\n",
        "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
        "g.map_diag(sns.histplot, kde_kws={'color': 'black'})\n",
        "g.map_upper(corrdot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFGcxNx0GJ1E"
      },
      "source": [
        "Looking at correlations between base pairs and loop types, we see a high correlation of -.82 between loop types E and S. It looks like there may be an strong grouping of data anchoring this relationship. There is also a strong inverse relationship of -.60 between U-A and G-C pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kM1r3HigY7C"
      },
      "source": [
        "Signal to noise ratio exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzxK4pq7gcF-"
      },
      "source": [
        "fig = px.histogram(\n",
        "    full_train, \n",
        "    \"signal_to_noise\", \n",
        "    nbins=25, \n",
        "    title='signal_to_noise histogram', \n",
        "    width=700,\n",
        "    height=500\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfx2dWjQgyf0"
      },
      "source": [
        "ds = full_train['SN_filter'].value_counts().reset_index()\n",
        "ds.columns = ['SN_filter', 'count']\n",
        "fig = px.pie(\n",
        "    ds, \n",
        "    values='count', \n",
        "    names=\"SN_filter\", \n",
        "    title='SN_filter pie chart', \n",
        "    width=500, \n",
        "    height=500\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT-i2ElbDUQd"
      },
      "source": [
        "Looking for correlation in the means of the prediction targets showed us correlation among the observations/samples. For example, observations with a higher mean deg_50_Mg_50C had a higher mean deg_ph10. The high degree of dimensionality of the target/output variables means that we'll need to employ modeling techquiques that can handle prediction of multiple outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIH14CQurgyk"
      },
      "source": [
        "full_train['mean_reactivity'] = full_train['reactivity'].apply(lambda x: np.mean(x))\n",
        "full_train['mean_deg_Mg_pH10'] = full_train['deg_Mg_pH10'].apply(lambda x: np.mean(x))\n",
        "full_train['mean_deg_Mg_50C'] = full_train['deg_Mg_50C'].apply(lambda x: np.mean(x))\n",
        "full_train['mean_deg_pH10'] = full_train['deg_pH10'].apply(lambda x: np.mean(x))\n",
        "full_train['mean_deg_50C'] = full_train['deg_50C'].apply(lambda x: np.mean(x))\n",
        "\n",
        "full_train_plots = full_train.iloc[:,19:25]\n",
        "\n",
        "full_train_plots\n",
        "\n",
        "sns.set(style='white', font_scale=1)\n",
        "g = sns.PairGrid(full_train_plots, aspect=1.4, diag_sharey=False)\n",
        "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
        "g.map_diag(sns.histplot, kde_kws={'color': 'black'})\n",
        "g.map_upper(corrdot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaVQ2n5BqMx6"
      },
      "source": [
        "## Data Augmentation \n",
        "\n",
        "### Tokenisation\n",
        "\n",
        "The key input data: sequence, structure and predicted loop type are all given in character (such as \"A\", '.'), which would be hard for algorithm to consume. Therefore, we tokenise the data and convert it into numerical values. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57_a6AI-_Ifs"
      },
      "source": [
        "\n",
        "# Create an dictionary to map character to a integer\n",
        "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
        "\n",
        "def tokenise(data,  columns = ['sequence','structure', 'predicted_loop_type'], dic = token2int ):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to convert sequence, structure and predited_loop_type into one long numerical \n",
        "  feature array for each RNA sample \n",
        "  \"\"\"\n",
        "\n",
        "  # grab the data \n",
        "  temp = list()\n",
        "  for col in columns:\n",
        "    temp = temp + list(data[col])\n",
        "  temp = np.array(temp)\n",
        "\n",
        "  # creating mapping array \n",
        "  k = np.array(list(dic.keys()))\n",
        "  v = np.array(list(dic.values()))\n",
        "\n",
        "  # Get argsort indices\n",
        "  sidx = k.argsort()\n",
        "\n",
        "  # map the initial array to integers \n",
        "  ks = k[sidx]\n",
        "  vs = v[sidx]\n",
        "  return vs[np.searchsorted(ks,temp)]\n",
        "\n",
        "# apply the function to train and test data \n",
        "full_train['feature_array'] = full_train.apply(lambda x: tokenise(x, dic = token2int), axis = 1)\n",
        "full_test['feature_array'] =public_test.apply(lambda x: tokenise(x, dic = token2int), axis = 1)\n",
        "\n",
        "\n",
        "# tokenise them into separate columns as well \n",
        "full_train['token_structure'] = full_train.apply(lambda x: tokenise(x, columns = ['structure']), axis = 1)\n",
        "full_train['token_looptype'] = full_train.apply(lambda x: tokenise(x, columns = ['predicted_loop_type']), axis = 1)\n",
        "full_test['token_structure'] = full_test.apply(lambda x: tokenise(x,  columns = ['structure']), axis = 1)\n",
        "full_test['token_looptype'] = full_test.apply(lambda x: tokenise(x, columns = ['predicted_loop_type']), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6gs6SfVmYt0"
      },
      "source": [
        "# for each sample, pair features of interest into each position\n",
        "def pair_features(data, features):\n",
        "  \"\"\" Data Engineering function, for each sequence, pair the features of interest to a 1D array, ex [feature1, feature2, feature3]/\n",
        "  \"\"\"\n",
        "  output_df = pd.DataFrame()\n",
        "  output_df[\"id\"] = data[\"id\"]\n",
        "\n",
        "  # type the columns to list, only use first 68 positions\n",
        "  for feature in features:\n",
        "    output_df[feature] = data[feature].apply(lambda x: list(x)[:68])\n",
        "  output_df = output_df.set_index(['id']).apply(pd.Series.explode).reset_index()\n",
        "  output_df[\"feature_paird\"] = output_df[features].values.tolist()\n",
        "  output = output_df.groupby(\"id\")[\"feature_paird\"].apply(list).reset_index(name='paird_feature')\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSEhm6a0o6PQ"
      },
      "source": [
        "# Checking if the shape is as expected \n",
        "np.array(full_train['feature_array'].tolist()).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXwtC6GrtCc_"
      },
      "source": [
        "### Train/ test/ dev split \n",
        "\n",
        "We split training data into 80% training, 10% development and 10% test data. So we can evaluate and compare the model before choosing a final model for submission. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu9RiZtzo7MT"
      },
      "source": [
        " # split into a training, a dev and and a test data\n",
        "train_data, dev_test = train_test_split(\n",
        "    full_train, test_size=.2, random_state=34, stratify = full_train.SN_filter)\n",
        " \n",
        "dev_data, test_data = train_test_split(\n",
        "    dev_test, test_size=.5, random_state=34, stratify = dev_test.SN_filter)\n",
        "\n",
        "dev_data.shape \n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uje9FdOrEfh"
      },
      "source": [
        "## Evaluation score\n",
        "\n",
        "The same evaluation score should be used consistently across to assess the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_VQNoZsq85N"
      },
      "source": [
        "# evaluation score\n",
        "def MCRMSE(y_true, y_pred):\n",
        "  if y_true.shape[1:3] == (68,5): \n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=(0, 1))\n",
        "  elif y_true.shape[1:3] == (5,68): \n",
        "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=(0, 2))\n",
        "  else: \n",
        "    raise Exception('Unexpected shape')\n",
        "  return tf.reduce_mean(tf.sqrt(colwise_mse), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlSiEbNpr0H5"
      },
      "source": [
        "## Baseline model - vanilla neural network\n",
        "\n",
        "We are chosing a vanilla neural network as a baseline model. The reason for the choice was because of the input data we have. As mentioned in the data augementation section, the main input data was tokenised biological representation of RNA, it would be quite meaningless for some statistical model since the the numbers does not bear any nmerical/ categorical meaning. We think a neural network should be able to handle the complexity inside the problem. General field research and browsing through past submission also proven the choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVR7E8hInVw0"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def build_sklearn_model(input_feature, output_target, model, params,  dev_feature, dev_target):\n",
        "  \"\"\"\n",
        "  Training function to conduct grid search for sklearn package model \n",
        "  \"\"\"    \n",
        "  # def a grid search for var_smoothing \n",
        "\n",
        "  # start a grid search with 5 fold CV\n",
        "  clf = GridSearchCV(model, params, cv=5, \n",
        "                     n_jobs = 2,\n",
        "                     scoring = 'neg_root_mean_squared_error')\n",
        "  clf.fit(input_feature, output_target)\n",
        "\n",
        "  # get the best performing model from grid search \n",
        "  clf_best = clf.best_estimator_\n",
        "  clf_best.fit(input_feature, output_target)\n",
        "  \n",
        "  return clf_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ8TRRm7wmqQ"
      },
      "source": [
        "Start tratining the model. We made a concise decision here that to build one single model for all target output. \n",
        "1. Given the amount of the data, it was quite computationally and timely consuming to train the model. Having one model for 5 targets could control the overall expense. \n",
        "2. Sophisticated model like NN should be able to handle multiple output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiYybEI6KFKk"
      },
      "source": [
        "# prep feature \n",
        "train_feature = np.array(train_data['feature_array'].tolist())\n",
        "dev_feature = np.array(dev_data['feature_array'].tolist())\n",
        "test_feature = np.array(test_data['feature_array'].tolist())\n",
        "\n",
        "\n",
        "# for target in targets: \n",
        "  # prep the data\n",
        "output_target = np.array(train_data[targets].values.tolist()).reshape((1920, 68*5))\n",
        "dev_target =np.array(dev_data[targets].values.tolist()).reshape((240, 68*5))\n",
        "test_target = np.array(test_data[targets].values.tolist()).reshape((240, 68*5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jItgwwxQFTK"
      },
      "source": [
        " np.array(train_data[targets].values.tolist()).shape[1:3] == (5,68)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqzvqf8KQz9"
      },
      "source": [
        "# start grid search for all desired outputs\n",
        "targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
        "\n",
        "\n",
        "# initiate a single ANN\n",
        "clf_single = MLPRegressor(max_iter=300, learning_rate = 'adaptive')\n",
        "       \n",
        "#def a set of params to use for grid search \n",
        "params = {'alpha': [ 0.0001, 0.001, 0.01, 0.1, 1.0, 2.0, 10.0],\n",
        "          'hidden_layer_sizes': [(100,), (200,), (100, 100), (100, 50, 50, 50)]\n",
        "          }\n",
        "\n",
        "  # start grid search\n",
        "nn_estimator = build_sklearn_model(train_feature, output_target, clf_single, params, dev_feature, dev_target)\n",
        "\n",
        "# and include prediction\n",
        "nn_predict = nn_estimator.predict(test_feature)\n",
        "\n",
        "# # It takes really long time to do the gridsearch, so make some noise when it finish\n",
        "# import IPython\n",
        "# display(IPython.display.Audio(url=\"https://www.soundhelix.com/examples/mp3/SoundHelix-Song-14.mp3\", autoplay=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIkYRxxF05Cs"
      },
      "source": [
        "nn_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNulMgKZBstG"
      },
      "source": [
        "#### Baseline score\n",
        "\n",
        "As mentioned, we will use the same metric as the competition is using to evaluate the score, a column wise root mean square error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y5kVYFFdH_5"
      },
      "source": [
        "baseline_score = MCRMSE(nn_predict.reshape(240,5,68), test_target.reshape(240,5,68))\n",
        "print(f'The baseline score is {baseline_score.numpy()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le21-gUB7XYW"
      },
      "source": [
        "#### Baseline prediction result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEQRY2Kr4NDR"
      },
      "source": [
        "# result = pd.DataFrame(dict(zip(targets, nn_predict)), index=test_data.id)\n",
        "# result.to_csv('/content/drive/MyDrive/1 MIDS/W207 Applied Machine Learning/Final Project/base_line_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bHsIjjEkAK"
      },
      "source": [
        "## Some thoughts on next steps\n",
        "1. the best performing models in the competition submissions are variants of LSTM, which makes sense as we always want to previous bases in a sequence would matter, should we should take a look\n",
        "2. the bpps data is worthy exprimenting.\n",
        "3. the sequence been scored is a shorter set compared to sequence provieded, maybe we can throw away the bottom part to reduce the dimension.\n",
        "4. more feature engineering, using the base pairs for e.g.\n",
        "    - explore relationship of base pairs with prediction targets in more granular detail. For example are G bases associated with higher reactivity?\n",
        "5. Use the competition specific scoring metric to evaluate performance.\n",
        "6. We used seperate models for 5 output variables, since each output is already non-scalar, we can also try to predict all 5 output with one model.\n",
        "7. The ANN are suffering from covergence warmings despite a large epoches. We could try other optimisation methods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iHAZtQgv0R4"
      },
      "source": [
        "## More model expriment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7E5YHRcss-"
      },
      "source": [
        "### Support Vector Regression\n",
        "\n",
        "Support vector has been popular to model for none-linear relationship, we would like to expriment the possibility with it. However, SVR by default only output one prediction in each model. Fortunately, given sklearn provides wrappers for multiple outputs so we were still able to expriment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrXhTKqDcpCd"
      },
      "source": [
        "\n",
        "# initiate a single support vector machine. \n",
        "svr_single = SVR(kernel = 'sigmoid', tol=0.01)\n",
        "mor = MultiOutputRegressor(svr_single)\n",
        "       \n",
        "#def a set of params to use for grid search \n",
        "params = {'estimator__C': [ 0.0001, 0.001, 0.01, 0.1, 1.0, 2.0, 10.0]\n",
        "          }\n",
        "\n",
        "# start grid search\n",
        "svr_estimator = build_sklearn_model(train_feature, output_target, mor, params, dev_feature, dev_target)\n",
        "\n",
        "# and include prediction\n",
        "svr_predictions = svr_estimator.predict(test_feature)\n",
        "# svr_predictions.append(list(target_predict))\n",
        "\n",
        "# score = np.sqrt(metrics.mean_squared_error(test_target, target_predict))\n",
        "# svr_scores.append([score])\n",
        "# print(f'Final SVR model for has RMSE: {score}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeT_V4jw074z"
      },
      "source": [
        "svr_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-XR6PnjqTLv"
      },
      "source": [
        "svr_score = MCRMSE(svr_predictions.reshape(240,5,68), test_target.reshape(240,5,68))\n",
        "\n",
        "print(f'The SVR score is {svr_score.numpy()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQDrKA1w4If_"
      },
      "source": [
        "### Recurrent Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xXxQb6VmM4K"
      },
      "source": [
        "Below is a a helper function to train and evaluate tensorflow models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkqRWk0EzUeD"
      },
      "source": [
        "def build_tensorflow_model(x_train, y_train, x_dev, y_dev, x_test, y_test, model, epoch =45):\n",
        "  \"\"\" a helper function to train and evaluate tensorflow models\"\"\"\n",
        "  \n",
        "  print('Model Summary')\n",
        "  print(model.summary())\n",
        "  print('\\n \\n')\n",
        "  print('Start training ')\n",
        "\n",
        "  hist = model.fit(x_train, y_train,  epochs=epoch, verbose = 2, validation_data=(x_dev, y_dev))\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.plot(hist.history['loss'], label = \"loss\")\n",
        "  plt.plot(hist.history['val_loss'], label = \"val_loss\")\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('MCRMSE')\n",
        "  plt.title('Training Histroy')\n",
        "\n",
        "  plt.legend()\n",
        "  # Display a figure.\n",
        "  plt.show()\n",
        "\n",
        "  print('Generate prediction')\n",
        "  predict = model.predict(x_test)\n",
        "  predicted_score = MCRMSE(predict, y_test)\n",
        "  print(f'The model testomh prediction score is {predicted_score.numpy()}')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bx5nCAImxAj"
      },
      "source": [
        "#### RNN1 - single target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUE0lA6zvw8x"
      },
      "source": [
        "This model only uses the features that derived from sequence but directly related to degradability at RNA positions. Structure and looptype are considered secondary structures. In this model, we are looking at the contributions of above two features and the base pairing probabilities and how they affect the degradation rate at each position synergically through a RNN sequence-to-sequence model. This way we assume that ~3000 RNAs are randomly sampled from  population, and fundamentally we assume all RNA with given features will result in the similar degradation rate in the targeted experimental conditions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyoVXPVz4L2C"
      },
      "source": [
        "features = [\"token_structure\", \"token_looptype\", \"bpps_average\"]\n",
        "targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
        "\n",
        "x_train_df = pair_features(train_data, features)\n",
        "x_dev_df = pair_features(dev_data, features)\n",
        "x_test_df = pair_features(test_data, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPDxw2Fkl3Nz"
      },
      "source": [
        "# convert to tf object and reshape to \n",
        "x_train = tf.convert_to_tensor(np.array(x_train_df['paird_feature'].tolist()).reshape((-1,68,3)))\n",
        "x_dev = tf.convert_to_tensor(np.array(x_dev_df['paird_feature'].tolist()).reshape((-1,68,3)))\n",
        "x_test = tf.convert_to_tensor(np.array(x_test_df['paird_feature'].tolist()).reshape((-1,68,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33vb03Wdl38f"
      },
      "source": [
        "def gru_layer(hidden_dim, dropout):\n",
        "    return layers.Bidirectional(layers.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
        "def LSTM_layer(hidden_dim, dropout):\n",
        "    return layers.Bidirectional(layers.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='glorot_uniform'))\n",
        "\n",
        "# Modeling\n",
        "model6 = Sequential()\n",
        "model6.add(layers.Input(shape=(68, 3)))\n",
        "model6.add(layers.Embedding(input_dim=len(x_train)+1, output_dim=1))\n",
        "model6.add(layers.Reshape(target_shape=(68,3)))\n",
        "model6.add(layers.SpatialDropout1D(rate=0.2))\n",
        "model6.add(LSTM_layer(hidden_dim=256, dropout=0.2))\n",
        "model6.add(gru_layer(hidden_dim=256, dropout=0.2))\n",
        "model6.add(layers.Dense(1, activation='sigmoid'))\n",
        "model6.compile(optimizer=\"Adam\", loss=MCRMSE, metrics=[\"mse\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2MDuJYsl7Qw"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXwHcbSyl70W"
      },
      "source": [
        "for label in targets:\n",
        "  y_train = tf.convert_to_tensor(np.array(train_data[label].tolist()).reshape((-1,68,1)))\n",
        "  y_dev = tf.convert_to_tensor(np.array(dev_data[label].tolist()).reshape((-1,68,1)))\n",
        "  y_test = tf.convert_to_tensor(np.array(test_data[label].tolist()).reshape((-1,68,1)))\n",
        "  print(\"Target: \" + label + \": \\n\")\n",
        "  build_tensorflow_model(x_train, y_train, x_dev, y_dev, x_test, y_test, model6, epoch = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QRAqVmjMq1M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYRnPhSow0Ib"
      },
      "source": [
        "### More RNN - multiple targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDWejYSVytr_"
      },
      "source": [
        "#### RNN2 \n",
        "The thoughts here is that a large amount of bpps data was derived from the sequence/ loop type/ structure data. Since they are derived data, We want to expriment repoducing those from sequences with neural networks. Therefore, we will add multiple middle layers first to generate more data, potentially to mimic the other data that are provided, and then extract the information from it. We again want to use RNN given the sequential nature of the data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWAYsegEyIDZ"
      },
      "source": [
        "# convert to tf object\n",
        "train_sequence = tf.convert_to_tensor(np.array(train_data['feature_array'].tolist()).reshape((-1,1,321)))\n",
        "dev_sequence = tf.convert_to_tensor(np.array(dev_data['feature_array'].tolist()).reshape((-1,1,321)))\n",
        "test_sequence = tf.convert_to_tensor(np.array(test_data['feature_array'].tolist()).reshape((-1,1,321)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOl55hvhypiY"
      },
      "source": [
        "# prepare train/dev label (y)\n",
        "y_train_all = tf.convert_to_tensor(np.array(train_data[targets].values.tolist()))\n",
        "y_dev_all = tf.convert_to_tensor(np.array(dev_data[targets].values.tolist()))\n",
        "y_test_all = tf.convert_to_tensor(np.array(test_data[targets].values.tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifoTUdD9zVAT"
      },
      "source": [
        "# Initialising a model \n",
        "model2 = Sequential()\n",
        "\n",
        "# using Embedding as first layer since all \n",
        "model2.add(layers.Embedding(len(token2int)+1, 1, input_shape = (1, 321)))\n",
        "model2.add(layers.Reshape((1,321)))\n",
        "# Recurrent layer, using a GRU since it is cheaper \n",
        "model2.add(layers.GRU(321, return_sequences=True, recurrent_dropout=0.05))\n",
        "# # The choice of the recurrent unit is 68*68, same size the bpps file size for each sample \n",
        "model2.add(Dense(4624, activation='sigmoid'))\n",
        "model2.add(Dropout(0.05, seed = 34))\n",
        "\n",
        "model2.add(layers.Conv1D(2312, 1, activation='linear', \n",
        "                  kernel_initializer='glorot_uniform',\n",
        "                  bias_initializer='glorot_uniform'))\n",
        "# Some fully connected layer\n",
        "model2.add(Dense(1024, activation='sigmoid'))\n",
        "model2.add(layers.MaxPooling1D(1,1))\n",
        "model2.add(Dense(512, activation='sigmoid'))\n",
        "model2.add(layers.LSTM(340, dropout=0.05, recurrent_dropout=0.05,))\n",
        "model2.add(layers.Reshape((5,68)))\n",
        "model2.add(Dense(68, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "model2.add(Dense(68, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(momentum=0.9)\n",
        "# Use the loss function by the scoring method \n",
        "model2.compile(optimizer=opt, loss=MCRMSE, metrics=[\"mse\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQhIzzYO6fVR"
      },
      "source": [
        "model2 = build_tensorflow_model(train_sequence, y_train_all, dev_sequence, y_dev_all, test_sequence, y_test_all, model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz63Oj-Y2chG"
      },
      "source": [
        "# print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGJWy51hz9YT"
      },
      "source": [
        "# model2.fit(train_sequence, y_train_all,  epochs=30,  validation_data=(dev_sequence, y_test_all))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgelPlZ_XajT"
      },
      "source": [
        "# model2_predict = model2.predict(test_sequence)\n",
        "# MCRMSE(model2_predict, y_test_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkrDR8Kj8Idm"
      },
      "source": [
        "#### RNN3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joITLp8kGrq6"
      },
      "source": [
        "# Initialising a model \n",
        "model3 = Sequential()\n",
        "\n",
        "# using Embedding as first layer since all \n",
        "model3.add(layers.Embedding(len(token2int)+1, 1, input_shape = (1, 321)))\n",
        "model3.add(layers.Reshape((1,321)))\n",
        "# Recurrent layer, using a GRU since it is cheaper \n",
        "model3.add(layers.GRU(512, return_sequences=True, recurrent_dropout=0.05))\n",
        "# # The choice of the recurrent unit is 68*68, same size the bpps file size for each sample \n",
        "model3.add(Dropout(0.05, seed = 34))\n",
        "\n",
        "# model3.add(layers.Conv1D(512, 1, activation='linear', \n",
        "#                   kernel_initializer='glorot_uniform',\n",
        "#                   bias_initializer='glorot_uniform'))\n",
        "# Some fully connected layer\n",
        "# model3.add(Dense(512, activation='sigmoid'))\n",
        "# model3.add(layers.MaxPooling1D(1,1))\n",
        "model3.add(layers.LSTM(340, dropout=0.05, recurrent_dropout=0.05,))\n",
        "model3.add(layers.Reshape((5,68)))\n",
        "\n",
        "# Output layer\n",
        "model3.add(Dense(68, activation='linear'))\n",
        "model3.add(Dense(68, activation='tanh'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(momentum=0.9)\n",
        "# Use the loss function by the scoring method \n",
        "model3.compile(optimizer=opt, loss=MCRMSE, metrics=[\"mse\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ug4Hc0t6gp9"
      },
      "source": [
        "model3 = build_tensorflow_model(train_sequence, y_train_all, dev_sequence, y_dev_all, test_sequence, y_test_all, model3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACQ-BUbK3uli"
      },
      "source": [
        "#### RNN4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejp8gah3w0rS"
      },
      "source": [
        "# Initialising a model \n",
        "model4 = Sequential()\n",
        "\n",
        "# using Embedding as first layer since all \n",
        "model4.add(layers.Embedding(len(token2int)+1, 1, input_shape = (1, 321)))\n",
        "model4.add(layers.Reshape((1,321)))\n",
        "model4.add(Dropout(0.05, seed = 34))\n",
        "# Recurrent layer, using a GRU since it is cheaper \n",
        "model4.add(layers.Bidirectional(layers.GRU(107, return_sequences=True, recurrent_dropout=0.05)))\n",
        "model4.add(layers.Bidirectional(layers.GRU(107, return_sequences=True, recurrent_dropout=0.05)))\n",
        "\n",
        "# # The choice of the recurrent unit is 68*68, same size the bpps file size for each sample \n",
        "model4.add(Dropout(0.05, seed = 34))\n",
        "\n",
        "# Some fully connected layer\n",
        "model4.add(layers.Bidirectional(layers.LSTM(340, dropout=0.05, recurrent_dropout=0.05,)))\n",
        "# model4.add(layers.Bidirectional(layers.LSTM(340, dropout=0.05, recurrent_dropout=0.05,)))\n",
        "model4.add(Dense(340, activation='sigmoid'))\n",
        "model4.add(layers.Reshape((5,68)))\n",
        "\n",
        "# # Output layer\n",
        "model4.add(Dense(68, activation='linear'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(momentum=0.9)\n",
        "# Use the loss function by the scoring method \n",
        "model4.compile(optimizer=opt, loss=MCRMSE, metrics=[\"mse\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfriSH-cxTNt"
      },
      "source": [
        "model4 = build_tensorflow_model(train_sequence, y_train_all, dev_sequence, y_dev_all, test_sequence, y_test_all, model4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2VpzGVaxiiz"
      },
      "source": [
        "# model4.fit(train_sequence, y_train_all,  epochs=75,  validation_data=(dev_sequence, y_dev_all))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J3cs1IHyNaT"
      },
      "source": [
        "# model4_predict = model4.predict(test_sequence)\n",
        "# MCRMSE(model4_predict, y_test_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOq22Tzi9WuA"
      },
      "source": [
        "#### RNN5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0wfHp7wgfWV"
      },
      "source": [
        "train_sequence2 = tf.convert_to_tensor( np.array(train_data['feature_array'].tolist()).reshape((-1,3,107)).swapaxes(1,2))\n",
        "dev_sequence2 = tf.convert_to_tensor(np.array(dev_data['feature_array'].tolist()).reshape((-1,3,107)).swapaxes(1,2))\n",
        "test_sequence2 = tf.convert_to_tensor( np.array(test_data['feature_array'].tolist()).reshape((-1,3,107)).swapaxes(1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VvR_2hJaUCa"
      },
      "source": [
        "y_train_all2 = tf.convert_to_tensor(np.array(train_data[targets].values.tolist()).swapaxes(1,2))\n",
        "y_dev_all2 = tf.convert_to_tensor(np.array(dev_data[targets].values.tolist()).swapaxes(1,2))\n",
        "y_test_all2 = tf.convert_to_tensor(np.array(test_data[targets].values.tolist()).swapaxes(1,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v7A1UOrgv2l"
      },
      "source": [
        "# Initialising a model \n",
        "\n",
        "inputs = layers.Input(shape=(107, 3))\n",
        "\n",
        "# using Embedding as first layer since all \n",
        "embed = layers.Embedding(len(token2int), 200) (inputs)\n",
        "embed = layers.Reshape((107,600))(embed)\n",
        "# Recurrent layer, using a GRU since it is cheaper \n",
        "hidden = layers.SpatialDropout1D(0.05, seed = 34)(embed)\n",
        "\n",
        "\n",
        "hidden = layers.Bidirectional(layers.GRU(321, return_sequences=True, recurrent_dropout=0.05))(embed)\n",
        "\n",
        "# # The choice of the recurrent unit is 68*68, same size the bpps file size for each sample \n",
        "hidden = layers.SpatialDropout1D(0.05, seed = 34)(hidden)\n",
        "\n",
        "# # Some fully connected layer\n",
        "hidden = layers.Bidirectional(layers.GRU(256, return_sequences=True, recurrent_dropout=0.05,kernel_initializer='orthogonal'))(hidden)\n",
        "\n",
        "hidden = hidden[:, :68]\n",
        "\n",
        "hidden = layers.Dense(60, activation='sigmoid')(hidden)\n",
        "\n",
        "# # # # # Output layer\n",
        "out = layers.Dense(5, activation='linear')(hidden)\n",
        "\n",
        "\n",
        "model5 = tf.keras.Model(inputs=inputs, outputs=out)\n",
        "model5.compile(tf.optimizers.Adam(), loss=MCRMSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsgWCrmheIpN"
      },
      "source": [
        "model5 = build_tensorflow_model(train_sequence2, y_train_all2, dev_sequence2, y_dev_all2, test_sequence2, y_test_all2, model5,epoch=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbD3jssMfNfk"
      },
      "source": [
        "inputs = layers.Input(shape=(107, 3))\n",
        "\n",
        "# using Embedding as first layer since all \n",
        "embed = layers.Embedding(len(token2int), 200) (inputs)\n",
        "embed = layers.Reshape((107,600))(embed)\n",
        "# Recurrent layer, using a GRU since it is cheaper \n",
        "hidden = layers.SpatialDropout1D(0.05, seed = 34)(embed)\n",
        "\n",
        "\n",
        "hidden = layers.Bidirectional(layers.GRU(321, return_sequences=True, recurrent_dropout=0.05))(embed)\n",
        "\n",
        "# # The choice of the recurrent unit is 68*68, same size the bpps file size for each sample \n",
        "hidden = layers.SpatialDropout1D(0.05, seed = 34)(hidden)\n",
        "\n",
        "# # Some fully connected layer\n",
        "hidden = layers.Bidirectional(layers.LSTM(256, return_sequences=True, recurrent_dropout=0.05,kernel_initializer='orthogonal'))(hidden)\n",
        "hidden = layers.Bidirectional(layers.LSTM(256, return_sequences=True, recurrent_dropout=0.05,kernel_initializer='orthogonal'))(hidden)\n",
        "\n",
        "hidden = hidden[:, :68]\n",
        "\n",
        "hidden = layers.Dense(60, activation='sigmoid')(hidden)\n",
        "\n",
        "# # # # # Output layer\n",
        "out = layers.Dense(5, activation='linear')(hidden)\n",
        "\n",
        "\n",
        "model6 = tf.keras.Model(inputs=inputs, outputs=out)\n",
        "model6.compile(tf.optimizers.Adam(), loss=MCRMSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paOlmlBSfZb0"
      },
      "source": [
        "model6 = build_tensorflow_model(train_sequence2, y_train_all2, dev_sequence2, y_dev_all2, test_sequence2, y_test_all2, model6,epoch=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWFoAtuDq2jk"
      },
      "source": [
        "## Exploratory unsupervised learning\n",
        "\n",
        "The datasets contain at least 107 different potential features over at least 3 different categories. The more raw BPPS data contains 107*107 features. As the dimensionality of this data is so high, we chose to see if there was value in reducing the dimensionality of the data for use in more interpretable models like k-means, knn, gmm, regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8EsEayE6chY"
      },
      "source": [
        "We started by dimensionally reducing the tokenized data and saw that PCA dimension reduction did not seem to explain the variance in the data, implying a low degree of redundancy in the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9ODmVwA29cO"
      },
      "source": [
        "# PCA on tokenized inputs feature array\n",
        "features_token = np.array(full_train['feature_array'].values.tolist()).reshape((2400, 321))\n",
        "# Initialize PCA\n",
        "pca = PCA(n_components=51)\n",
        "# Fit PCA\n",
        "principalComponents = pca.fit_transform(features_token)\n",
        "# Create data frame of principal components\n",
        "principalDf = pd.DataFrame(data = principalComponents)\n",
        "# Create array of varaciance explained by each factor\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "# display(variance_explained)\n",
        "# Iniitalize sensitivity analysis for different numbers of factors\n",
        "k = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
        "for component in k:\n",
        "  print(\"The fraction of the total variance in the train data explained by\",component, \" component(s) is\",round(variance_explained[component],3))\n",
        "\n",
        "# display(principalDf)\n",
        "# Plot number of principal components vs. cumulative variance explaiend\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xi = np.arange(1, 52, step=1)\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.ylim(0.0,1.1)\n",
        "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "plt.xlabel('Number of Components')\n",
        "plt.xticks(np.arange(0, 51, step=1))\n",
        "plt.ylabel('Cumulative variance (%)')\n",
        "plt.title('The number of components needed to explain variance for tokenized inputs')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "ax.grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp1MZr3Y6odv"
      },
      "source": [
        "So we decided to add more features to look for more explanatory power. We averaged the BPPS data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MMvTiUsscxj"
      },
      "source": [
        "# PCA on Average BPPS data\n",
        "# display(full_train)\n",
        "\n",
        "# Create an array of just the average bpps values\n",
        "bpps_avg_pca = np.array(full_train['feature_array'].values.tolist()).reshape((2400, 321))\n",
        "# Scale bpps_avg values between 0 and 1\n",
        "bpps_avg_pca_scaled = StandardScaler().fit_transform(bpps_avg_pca)\n",
        "# display(bpps_avg_pca)\n",
        "# print(len(bpps_avg_pca))\n",
        "# Initialize PCA\n",
        "pca = PCA(n_components=51)\n",
        "# Fit PCA\n",
        "principalComponents = pca.fit_transform(bpps_avg_pca_scaled)\n",
        "# Create data frame of principal components\n",
        "principalDf = pd.DataFrame(data = principalComponents)\n",
        "# Create array of varaciance explained by each factor\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "# display(variance_explained)\n",
        "# Iniitalize sensitivity analysis for different numbers of factors\n",
        "k = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
        "for component in k:\n",
        "  print(\"The fraction of the total variance in the train data explained by\",component, \" component(s) is\",round(variance_explained[component],3))\n",
        "\n",
        "# display(principalDf)\n",
        "# Plot number of principal components vs. cumulative variance explaiend\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xi = np.arange(1, 52, step=1)\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.ylim(0.0,1.1)\n",
        "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "plt.xlabel('Number of Components')\n",
        "plt.xticks(np.arange(0, 51, step=1))\n",
        "plt.ylabel('Cumulative variance (%)')\n",
        "plt.title('The number of components needed to explain variance average BPPS')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "ax.grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rvvfMn26x4b"
      },
      "source": [
        "That didn't seems to help, so we ran PCA on the raw 107*107 BPPS file and did not see an improvement in variance explainability. This implies that the input data has high dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sUyHt2sAY3"
      },
      "source": [
        "# Create a dataframe by reshaping the raw, full BPPS data\n",
        "single = np.array(full_train['bpps_full'].values.tolist()).reshape((2400, 107*107))\n",
        "single_df = pd.DataFrame(single)\n",
        "\n",
        "# Initialize PCA\n",
        "pca = PCA(n_components=51)\n",
        "# Fit PCA\n",
        "principalComponents = pca.fit_transform(single)\n",
        "# Create data frame of principal components\n",
        "principalDf = pd.DataFrame(data = principalComponents)\n",
        "# Create array of varaciance explained by each factor\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "# display(variance_explained)\n",
        "# Iniitalize sensitivity analysis for different numbers of factors\n",
        "k = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50]\n",
        "for component in k:\n",
        "  print(\"The fraction of the total variance in the train data explained by\",component, \" component(s) is\",round(variance_explained[component],3))\n",
        "\n",
        "# display(principalDf)\n",
        "# Plot number of principal components vs. cumulative variance explaiend\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xi = np.arange(1, 52, step=1)\n",
        "y = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.ylim(0.0,1.1)\n",
        "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
        "\n",
        "plt.xlabel('Number of Components')\n",
        "plt.xticks(np.arange(0, 51, step=1))\n",
        "plt.ylabel('Cumulative variance (%)')\n",
        "plt.title('The number of components needed to explain variance for full BPPS data')\n",
        "\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
        "\n",
        "ax.grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf6qoq3qshEg"
      },
      "source": [
        "# Initialize PCA w/ 2 components\n",
        "pca = PCA(n_components=50)\n",
        "# Fit PCA\n",
        "principalComponents = pca.fit_transform(single)\n",
        "# Create data frame of principal components\n",
        "principalDf = pd.DataFrame(data = principalComponents)\n",
        "# Create data frame of train label\n",
        "# train_df = pd.DataFrame(train_labels, columns = [\"target\"])\n",
        "# Concatenate train labels with PCs\n",
        "# finalDf = pd.concat([principalDf, train_df], axis = 1)\n",
        "# display(finalDf)\n",
        "\n",
        "# Plot principal componet 1 vs. 2 and display poison vs. non-poison\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA for full BPPS', fontsize = 20)\n",
        "ax.scatter(principalDf[0], principalDf[1])\n",
        "# targets = [1, 0]\n",
        "# colors = ['r', 'g']\n",
        "# for target, color in zip(targets,colors):\n",
        "#     indicesToKeep = finalDf['target'] == target\n",
        "#     ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "#               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "#               , c = color\n",
        "#               , s = 50)\n",
        "# mylabels = ('poisonous','non-poisonous')\n",
        "# ax.legend(labels=mylabels)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkwQd30-7EZB"
      },
      "source": [
        "By visualizaing the clusters generated by k-means, we were looking for groupings, but really didn't find any."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsL-0tVGt8kh"
      },
      "source": [
        "# Initalize K-means\n",
        "kmeans_pca = KMeans(n_clusters = 6, init = 'k-means++', random_state = 42)\n",
        "# Fit k-means\n",
        "kmeans_pca_fit = kmeans_pca.fit(principalComponents)\n",
        "kmeans_pca_fit_predict = kmeans_pca.fit_predict(principalComponents)\n",
        "\n",
        "# display(kmeans_pca.labels_)\n",
        "\n",
        "# Create data frame of clusters\n",
        "k_clusters_array = kmeans_pca.labels_\n",
        "k_clusters = pd.DataFrame(kmeans_pca.labels_, columns = [\"clusters\"])\n",
        "\n",
        "# train_df = pd.DataFrame(train_labels, columns = [\"target\"])\n",
        "# clustered_pca = pd.concat([principalDf, k_clusters, train_df], axis=1)\n",
        "# display(clustered_pca)\n",
        "\n",
        "# Initialize dictionaries for centroids\n",
        "\n",
        "clusters_centroids=dict()\n",
        "clusters_radii= dict()\n",
        "\n",
        "# loop over clusters and calculate  distance of \n",
        "# each point within that cluster from its centroid and \n",
        "# pick the maximum which is the radius of that cluster\n",
        "# display(principalComponents)\n",
        "# display(set(k_clusters_array))\n",
        "# display(kmeans_pca_fit_predict)\n",
        "for cluster in list(set(k_clusters_array)):      \n",
        "    # print(cluster)\n",
        "    clusters_centroids[cluster]=list(zip(kmeans_pca.cluster_centers_[:, 0],kmeans_pca.cluster_centers_[:,1]))[cluster]\n",
        "    clusters_radii[cluster] = max([np.linalg.norm(np.subtract(i,clusters_centroids[cluster])) for i in zip(principalComponents[kmeans_pca_fit_predict == cluster, 0],principalComponents[kmeans_pca_fit_predict == cluster, 1])])\n",
        "# display(clusters_centroids)\n",
        "\n",
        "# Visualize the cluster with centroids and circles\n",
        "\n",
        "fig, ax = plt.subplots(1,figsize=(10,10))\n",
        "\n",
        "# Need to set aspect ratio to equal to get plots to come out circular\n",
        "ax.set_aspect(\"equal\")\n",
        "\n",
        "# Char labels\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('k-means applied to PCA full BPPS', fontsize = 20)\n",
        "\n",
        "# Plot clusters and circles\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 0, 0], principalComponents[kmeans_pca_fit_predict == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
        "art = mpatches.Circle(clusters_centroids[0],clusters_radii[0], edgecolor='r',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 1, 0], principalComponents[kmeans_pca_fit_predict == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "art = mpatches.Circle(clusters_centroids[1],clusters_radii[1], edgecolor='b',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 2, 0], principalComponents[kmeans_pca_fit_predict == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
        "art = mpatches.Circle(clusters_centroids[2],clusters_radii[2], edgecolor='g',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 3, 0], principalComponents[kmeans_pca_fit_predict == 3, 1], s = 100, c = 'purple', label = 'Cluster 4')\n",
        "art = mpatches.Circle(clusters_centroids[3],clusters_radii[3], edgecolor='purple',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 4, 0], principalComponents[kmeans_pca_fit_predict == 4, 1], s = 100, c = 'orange', label = 'Cluster 5')\n",
        "art = mpatches.Circle(clusters_centroids[4],clusters_radii[4], edgecolor='orange',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "plt.scatter(principalComponents[kmeans_pca_fit_predict == 5, 0], principalComponents[kmeans_pca_fit_predict == 5, 1], s = 100, c = 'black', label = 'Cluster 6')\n",
        "art = mpatches.Circle(clusters_centroids[5],clusters_radii[5], edgecolor='black',fill=False)\n",
        "ax.add_patch(art)\n",
        "\n",
        "#Plotting the centroids of the clusters\n",
        "plt.scatter(kmeans_pca.cluster_centers_[:, 0], kmeans_pca.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}